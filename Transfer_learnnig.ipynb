{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59a073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os,re\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import class_weight\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.nn.parallel import DataParallel\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70063279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e92068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this funcations concat all file names of faces\n",
    "folder_path = \"branddataset/images\"  \n",
    "file_list = os.listdir(folder_path)\n",
    "imgs=[]\n",
    "for filename in file_list:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        imgs.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ddd399e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44326"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30a9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"branddataset/brand_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0762e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ID\"]=df[\"ID\"].astype(str)\n",
    "df[\"ID\"]=df[\"ID\"]+\".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5910ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        39386.jpg\n",
       "1        59263.jpg\n",
       "2        53759.jpg\n",
       "3        29114.jpg\n",
       "4         9204.jpg\n",
       "           ...    \n",
       "15132    39336.jpg\n",
       "15133    46835.jpg\n",
       "15134    19731.jpg\n",
       "15135    50950.jpg\n",
       "15136    48193.jpg\n",
       "Name: ID, Length: 15137, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc2541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real=df[\"ID\"]\n",
    "\n",
    "def find_fake(large_arr,small_arr):\n",
    "    small_set=set(small_arr)\n",
    "    arr=[]\n",
    "    for x in large_arr:\n",
    "        if x not in small_set:\n",
    "            arr.append(x)\n",
    "    return arr\n",
    "\n",
    "not_real=find_fake(imgs,real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3bfcba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29200, 15137)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_real),len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a069f7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        39386.jpg\n",
       "1        59263.jpg\n",
       "2        53759.jpg\n",
       "3        29114.jpg\n",
       "4         9204.jpg\n",
       "           ...    \n",
       "15132    39336.jpg\n",
       "15133    46835.jpg\n",
       "15134    19731.jpg\n",
       "15135    50950.jpg\n",
       "15136    48193.jpg\n",
       "Name: ID, Length: 15137, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ed19137",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_y=np.ones(len(real))\n",
    "not_real_y=np.zeros(len(not_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "457593f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15137,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e6931a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29200,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_real_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cee1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4499f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cd600a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet18_model():\n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "    num_feat=resnet18.fc.in_features\n",
    "    resnet18.fc=nn.Softmax(2)\n",
    "    resnet18.train()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "    resnet18 = resnet18.to(device)\n",
    "    return resnet18,device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40add3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_img(image):\n",
    "    preprocess=transforms.Compose([transforms.ToTensor(),transforms.Resize((224, 224)),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    preprocessed_image = preprocess(image)\n",
    "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
    "    return preprocessed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47b089a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_resnet18_model(model, device, train_loader, n_epoch, lr):\n",
    "    print(\"loading model\")\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer=torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        running_loss=0.9\n",
    "        \n",
    "        for inputs,labels in train_loader:\n",
    "            inputs,labels=inputs.to(device),labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs=model(inputs)\n",
    "            loss=criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss+=loss.item()\n",
    "        epoch_loss=running_loss/len(train_loader)\n",
    "        print(f\"Epoch{epoch+1}/{num_epochs},Loss:{epoch_loss}:.4f\")\n",
    "    print(\"training finised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa86fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_model,device = load_resnet18_model()\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735abf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32699287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Step 1: Define a custom dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_dir_1,arr1, data_dir_2, arr2,transform=None):\n",
    "        self.data_dir_1 = data_dir_1\n",
    "        self.data_dir_2 = data_dir_2\n",
    "        class_1_set=set(arr1)\n",
    "        class_2_set=set(arr2)\n",
    "        self.class_1_files = sorted([f for f in os.listdir(data_dir_1) if f.endswith('.jpg') and f in class_1_set])\n",
    "        self.class_2_files = sorted([f for f in os.listdir(data_dir_2) if f.endswith('.jpg') and f in class_2_set])\n",
    "        self.labels = [0] * len(self.class_1_files) + [1] * len(self.class_2_files)\n",
    "        self.image_files = [os.path.join(data_dir_1, image_file) for image_file in self.class_1_files]+[os.path.join(data_dir_2, image_file) for image_file in self.class_2_files]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_files[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[index])\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Step 2: Create the custom dataset and DataLoader\n",
    "# Replace 'path/to/your/data_folder/' with the actual path to your data folder\n",
    "data_folder_path = \"branddataset/images/\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the image\n",
    "])\n",
    "\n",
    "custom_dataset = ImageDataset(data_folder_path,not_real, data_folder_path, real,transform=transform)\n",
    "\n",
    "# Define batch size and other DataLoader parameters\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "\n",
    "\n",
    "train_ratio = 0.8  # 80% of the data for training, adjust as needed\n",
    "train_size = int(train_ratio * len(custom_dataset))\n",
    "test_size = len(custom_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(custom_dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# # Step 5: Iterate through the DataLoaders to access batches of images and labels during training and testing\n",
    "# for images, labels in train_loader:\n",
    "\n",
    "#     pass\n",
    "\n",
    "# for images, labels in test_loader:\n",
    "\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/basantkaur/miniforge3/envs/tensorflow2/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/basantkaur/miniforge3/envs/tensorflow2/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'ImageDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e624cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
